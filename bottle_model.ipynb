{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajivDalal/hydration-essentials/blob/main/bottle_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading and Unzipping Dataset"
      ],
      "metadata": {
        "id": "MISnVpbqNeO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('archive.zip', 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "LAwAod1TNzaJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Partition"
      ],
      "metadata": {
        "id": "P89-SMzs_0O9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "mUrXTdkBK3Cr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_partitions_tf(ds, train_split=0.75, val_split=0.125, test_split=0.125, shuffle=True, shuffle_size=10000):\n",
        "    assert (train_split + val_split + test_split) == 1, \"Splits must sum to 1\"\n",
        "\n",
        "    ds_size = len(ds)\n",
        "\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(shuffle_size, seed=12)\n",
        "\n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "\n",
        "    train_ds = ds.take(train_size)\n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "\n",
        "    return train_ds, val_ds, test_ds\n",
        "\n",
        "def load_images_from_directory(directory, image_size=(256, 256), batch_size=32):\n",
        "    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory=directory,\n",
        "        labels='inferred',\n",
        "        shuffle=True,\n",
        "        image_size=image_size,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "image_directory = '/content/archive'\n",
        "dataset = load_images_from_directory(image_directory)\n",
        "\n",
        "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_ds)}\")\n",
        "print(f\"Validation dataset size: {len(val_ds)}\")\n",
        "print(f\"Test dataset size: {len(test_ds)}\")\n"
      ],
      "metadata": {
        "id": "joWSURL8lK1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d631edd-2779-450e-cb17-62226228adae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 486 files belonging to 3 classes.\n",
            "Train dataset size: 12\n",
            "Validation dataset size: 2\n",
            "Test dataset size: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Preprocessing"
      ],
      "metadata": {
        "id": "IC7vwylkExdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "MKUugWXxFScD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resizing and Rescaling\n",
        "resize_and_rescale = keras.Sequential([\n",
        "    keras.layers.experimental.preprocessing.Resizing(256, 256),\n",
        "    keras.layers.experimental.preprocessing.Rescaling(1.0/255)\n",
        "])"
      ],
      "metadata": {
        "id": "-Ol0EcRvFxwC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation\n",
        "data_augmentation = keras.Sequential([\n",
        "    keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    keras.layers.experimental.preprocessing.RandomRotation(0.2)\n",
        "])"
      ],
      "metadata": {
        "id": "9sHS4q40GO14"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y)\n",
        ").prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = val_ds.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y)\n",
        ").prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = test_ds.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y)\n",
        ").prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "VhXZ72qpGVdh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ],
      "metadata": {
        "id": "RaskT6ZpG0g9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR, filename= \"tensorflow.log\", filemode=\"w\")\n",
        "\n",
        "from keras import models,layers\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML"
      ],
      "metadata": {
        "id": "ow6ByqvAKb4A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Keras models and layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten"
      ],
      "metadata": {
        "id": "K_JZN8JyKsgB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (32, 256, 256, 3)\n",
        "num_classes = 3\n",
        "\n",
        "model = Sequential([\n",
        "    resize_and_rescale,\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.build(input_shape=input_shape)"
      ],
      "metadata": {
        "id": "oY7yydBhMlmP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "HWIvjuP0Mtc1",
        "outputId": "b7fdb0a1-3f81-427c-b68a-b3e608e94c8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential (Sequential)     (32, 256, 256, 3)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (32, 254, 254, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (32, 127, 127, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (32, 125, 125, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (32, 62, 62, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (32, 60, 60, 64)          36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (32, 30, 30, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (32, 28, 28, 64)          36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (32, 14, 14, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (32, 12, 12, 64)          36928     \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (32, 6, 6, 64)            0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (32, 4, 4, 64)            36928     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPoolin  (32, 2, 2, 64)            0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (32, 256)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (32, 64)                  16448     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (32, 3)                   195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 183747 (717.76 KB)\n",
            "Trainable params: 183747 (717.76 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "TcjQCn2uOmU9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ipLpcJ0mQThy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}