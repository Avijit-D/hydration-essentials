{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajivDalal/hydration-essentials/blob/main/bottle_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading and Unzipping Dataset"
      ],
      "metadata": {
        "id": "MISnVpbqNeO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('archive.zip', 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "LAwAod1TNzaJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Partition"
      ],
      "metadata": {
        "id": "P89-SMzs_0O9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "mUrXTdkBK3Cr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_partitions_tf(ds, train_split=0.75, val_split=0.125, test_split=0.125, shuffle=True, shuffle_size=10000):\n",
        "    assert (train_split + val_split + test_split) == 1, \"Splits must sum to 1\"\n",
        "\n",
        "    ds_size = len(ds)\n",
        "\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(shuffle_size, seed=12)\n",
        "\n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "\n",
        "    train_ds = ds.take(train_size)\n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "\n",
        "    return train_ds, val_ds, test_ds\n",
        "\n",
        "def load_images_from_directory(directory, image_size=(256, 256), batch_size=32):\n",
        "    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory=directory,\n",
        "        labels='inferred',\n",
        "        shuffle=True,\n",
        "        image_size=image_size,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "image_directory = '/content/archive'\n",
        "dataset = load_images_from_directory(image_directory)\n",
        "\n",
        "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_ds)}\")\n",
        "print(f\"Validation dataset size: {len(val_ds)}\")\n",
        "print(f\"Test dataset size: {len(test_ds)}\")\n"
      ],
      "metadata": {
        "id": "joWSURL8lK1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8d9b199-bea9-4af4-9e46-b03cb34ab487"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 486 files belonging to 3 classes.\n",
            "Train dataset size: 12\n",
            "Validation dataset size: 2\n",
            "Test dataset size: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Preprocessing"
      ],
      "metadata": {
        "id": "IC7vwylkExdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "MKUugWXxFScD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resizing and Rescaling\n",
        "resize_and_rescale = keras.Sequential([\n",
        "    keras.layers.experimental.preprocessing.Resizing(256, 256),\n",
        "    keras.layers.experimental.preprocessing.Rescaling(1.0/255)\n",
        "])"
      ],
      "metadata": {
        "id": "-Ol0EcRvFxwC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation\n",
        "data_augmentation = keras.Sequential([\n",
        "    keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    keras.layers.experimental.preprocessing.RandomRotation(0.2)\n",
        "])"
      ],
      "metadata": {
        "id": "9sHS4q40GO14"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y)\n",
        ").prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = val_ds.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y)\n",
        ").prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = test_ds.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y)\n",
        ").prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "VhXZ72qpGVdh"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ],
      "metadata": {
        "id": "RaskT6ZpG0g9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yN4wudaAGdXJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}